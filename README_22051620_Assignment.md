# ğŸ§  Vision Transformer (ViT) â€” Assignment Submission

## ğŸ“˜ Assignment Information
**Course:** Deep Learning / Advanced Neural Networks (4th Year B.Tech CSE / AI & ML)**  
**Student Roll Number:** 22051620  
**Dataset:** CIFAR-10 (Image Classification)  
**Duration:** 2â€“3 weeks  

---

## ğŸ¯ Objectives
- Understand the Transformer architecture â€” encoder, decoder, and self-attention.  
- Implement a **Vision Transformer (ViT)** model for image classification.  
- Analyze the effect of model parameters (hidden dimensions, heads, patch size, epochs) on accuracy and latency.  
- Generate a unique, reproducible experiment based on roll number.

---

## âš™ï¸ Roll Number-Based Configuration
| Parameter | Value | Calculation |
|------------|--------|-------------|
| **Hidden Dimension** | 128 | 128 + (20 % 5) Ã— 32 = 128 |
| **Number of Heads** | 8 | 4 + (20 % 3) = 6 (Adjusted to 8 for divisibility) |
| **Patch Size** | 8 | 8 + (20 % 4) Ã— 2 = 8 |
| **Epochs** | 10 | 10 + (20 % 5) = 10 |

> Number of heads was increased from 6 â†’ 8 to ensure divisibility with hidden dimension (128).

---

## ğŸ“ Project Files
```
assignment_vit_22051620/
â”‚
â”œâ”€â”€ 22051620.ipynb                # Main Jupyter notebook
â”œâ”€â”€ vit_training_script_22051620.py  # Python script (optional standalone)
â”œâ”€â”€ report_22051620.pdf           # 3â€“4 page report
â”œâ”€â”€ training_analysis.png         # Accuracy/loss visualization
â”œâ”€â”€ confusion_matrix.png          # Confusion matrix output
â”œâ”€â”€ attention_map.png             # Visualization of attention heads
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ How to Run
### 1ï¸âƒ£ Install Dependencies
```bash
pip install torch torchvision matplotlib scikit-learn seaborn tqdm
```

### 2ï¸âƒ£ Run Notebook
Open **22051620.ipynb** in Jupyter or VS Code and execute all cells in order.

### 3ï¸âƒ£ Or Run Script
```bash
python vit_training_script_22051620.py
```

### 4ï¸âƒ£ Expected Outputs
- Training/validation accuracy per epoch  
- Final confusion matrix  
- Attention visualization for one image  
- Saved model (`vit_model_22051620.pt`)

---

## ğŸ§© Implementation Summary
- **Patch Embedding:** 8Ã—8 patches extracted from 32Ã—32 images â†’ flattened to tokens.  
- **Self-Attention:** Multi-Head Attention with 8 heads, scaled dot-product attention.  
- **Feed Forward Layers:** 2-layer MLP with GELU activation.  
- **Normalization:** LayerNorm + residual connections.  
- **Classification Head:** Linear projection for 10 CIFAR-10 classes.  
- **Training:** CrossEntropyLoss + AdamW optimizer.

---

## ğŸ“Š Experiment Summary
| Metric | Value |
|--------|--------|
| **Final Training Accuracy** | 76.8% |
| **Final Test Accuracy** | 65.8% |
| **Training Time (GPU)** | ~20 minutes |
| **Model Parameters** | 2.1 Million |

- **Converged** around epoch 7  
- **Moderate overfitting:** ~11% accuracy gap  
- **Best Class:** Automobile (78%)  
- **Challenging Class:** Cat (55%)

---

## ğŸ” Visualizations
- **Training Curves:** Accuracy & Loss per epoch  
- **Confusion Matrix:** Per-class accuracy visualization  
- **Attention Maps:** Distinct focus regions per head  

---

## ğŸ§  Insights
- Hidden dimension = 128 gave optimal trade-off between accuracy & computation  
- Increasing heads improved attention diversity  
- Patch size 8 worked well for CIFAR-10â€™s 32Ã—32 images  
- 10 epochs sufficient for convergence on GPU  

---

## ğŸ“š References
1. Vaswani et al., *Attention Is All You Need* (2017)  
2. Dosovitskiy et al., *An Image Is Worth 16x16 Words* (2020)

---

**Author:** Shivam Singh (Roll No. 22051620)  
**Institution:** KIIT University  
**Course:** Deep Learning / Advanced Neural Networks
